# -*- coding: utf-8 -*-
"""edgnn_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BxQ7knA_je75IwfJS7oH65LQaCVwuLtQ
"""

import torch
import torch.nn as nn
from torch.nn import Linear

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# init trainable parameters
def weights_init(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)


# function reset parameters
#using https://github.com/guillaumejaume/edGNN/blob/master/utils/inits.py
def reset(nn):
    def _reset(item):
        if hasattr(item, 'reset_parameters'):
            item.reset_parameters()

    if nn is not None:
        if hasattr(nn, 'children') and len(list(nn.children())) > 0:
            for item in nn.children():
                _reset(item)
        else:
            _reset(nn)

def reset_graph(graph):
  if 'he' in graph.edata:
    del graph.edata['he']
  if 'hu' in graph.ndata:
    del graph.ndata['hu']
  if 'hv' in graph.ndata:
    del graph.ndata['hv']
  if 'm' in graph.ndata:
    del graph.ndata['m']
  if 'hm' in graph.ndata:
    del graph.ndata['hm']
     

#################################################################
# I've implemented using this link of dgl documentation:        #
# https://docs.dgl.ai/en/0.5.x/guide/message.html               #
#################################################################
# 'u' represents source nodes, 'v' represents destination nodes,#
# 'e' represents edges, 'm' represents message                  #
#################################################################
class edgnn_MessagePassing_Model(nn.Module):
    def __init__(self,graph,node_dim,edge_dim,output,activation=None,dropout=None,
                 batch_norm=False):
      super(edgnn_MessagePassing_Model, self).__init__()

      self.graph = graph
      # node dimension
      self.node_dim = node_dim
      # edge dimension
      self.edge_dim = edge_dim
      self.output = output
      self.activation = activation
      self.dropout = dropout

      #batch norm for graph classification
      self.batch_norm = batch_norm

      input_dim = 2 * self.node_dim
      input_dim = input_dim + self.edge_dim

      if self.batch_norm:
            self.batch = nn.BatchNorm1d(self.output)

      self.linear = nn.Linear(input_dim, self.output)
      if self.dropout:
        self.dropout = nn.Dropout(p=self.dropout)

      # Reinitialize learnable parameters.
      reset(self.linear)
      
      self.apply(weights_init)


    # message function
    def message_function(self, edges):
      
      message = torch.cat([edges.src['hu'],edges.data['he']],
                          dim=1)
      if self.dropout:
        message = self.dropout(message)

      return {'m': message}

    # reduce function
    def reduce_function(self, nodes):
      collect_message = torch.sum(nodes.mailbox['m'], dim=1)
      return {'hm': collect_message}

    # update function
    def node_update_function(self, nodes):
      h = torch.cat([nodes.data['hu'],nodes.data['hm']],
                    dim=1)
      h = self.linear(h)
      if self.activation:
        h = self.activation(h)
      if self.dropout:
        h = self.dropout(h)
      if self.batch_norm:
        h = self.batch(h)

      return {'hv': h}


    def forward(self, node_features, edge_features, graph):
      if graph is not None:
        self.graph = graph
      reset_graph(self.graph)
      self.graph.ndata['hu'] = node_features.to(device)
      self.graph.edata['he'] = edge_features.to(device)
      self.graph.update_all(self.message_function,
                        self.reduce_function,
                        self.node_update_function)

      h = self.graph.ndata.pop('hv')
      return h.to(device)